{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d76dc7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eirasf/GCED-AA3/blob/main/lab5/lab5.ipynb)\n",
    "\n",
    "# Lab5: Aprendizaje por refuerzo - Métodos Montecarlo y Temporal Difference\n",
    "\n",
    "En este laboratorio profundizaremos en los métodos de control del aprendizaje por refuerzo. En particular, nos centraremos en métodos tabulares que no necesitan disponer de un modelo de cómo funciona el entorno. Distinguiremos dos familias: métodos **Montecarlo** y métodos de **Temporal Difference**.\n",
    "\n",
    "Trabajaremos nuevamente con [Gym](https://www.gymlibrary.dev/), aunque esta vez utilizaremos el entorno [`Blackjack-v1`](https://gymnasium.farama.org/environments/toy_text/blackjack/#blackjack) que, como su nombre indica, simula una partida de Blackjack.\n",
    "\n",
    ">\n",
    ">  *¿Qué es el Blackjack?*\n",
    ">\n",
    ">Según [Wikipedia](https://es.wikipedia.org/wiki/Blackjack): \"El blackjack, también llamado veintiuno, es un juego de cartas, propio de los casinos con una o más barajas inglesas de 52 cartas sin los comodines, que consiste en sumar un valor lo más próximo a 21 pero sin pasarse. En un casino cada jugador de la mesa juega únicamente contra el crupier, intentando conseguir una mejor jugada que este. El crupier está sujeto a reglas fijas que le impiden tomar decisiones sobre el juego. Por ejemplo, está obligado a pedir carta siempre que su puntuación sume 16 o menos,diciendo siempre empieza la mano y obligado a plantarse si suma 17 o más. Las cartas numéricas suman su valor, las figuras suman 10 y el As vale 11 o 1, a elección del jugador. En el caso del crupier, los Ases valen 11 mientras no se pase de 21, y 1 en caso contrario. La mejor jugada es conseguir 21 con solo dos cartas, esto es con un As más carta de valor 10. Esta jugada se conoce como Blackjack o 21 natural. Un Blackjack gana sobre un 21 conseguido con más de dos cartas.\"\n",
    "\n",
    "En este caso, el entorno de OpenGym simula una partida de un solo jugador contra el crupier. Además de lo descrito anteriormente, debes saber que el agente conoce el valor de la primera carta del crupier, pero no del resto de cartas del crupier.\n",
    "\n",
    "Carguemos el entorno y explorémoslo brevemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d574641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si los paquetes no están instalados, hay que ejecutar estas líneas:\n",
    "#!pip install gymnasium\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "env = gym.make('Blackjack-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd21dda",
   "metadata": {},
   "source": [
    "Examinando las propiedades y métodos del objeto `env` con `dir()` podemos comprobar que hay dos variables que nos dan toda la información que necesitamos respecto al estado:\n",
    " - `env.player`: registra las cartas que tiene el jugador.\n",
    " - `env.dealer`: registra las cartas que tiene el crupier. Esta información no estará disponible para el agente.\n",
    "\n",
    "Puedes acceder a estas variables como `env.get_wrapper_attr('dealer')` y `env.get_wrapper_attr('player')` respectivamente.\n",
    "  \n",
    "Podemos probar, además, a mostrar el entorno con el método render. Al haber definido el `render_mode` como `rgb_array`, podemos utilizar `matplotlib` para mostrarlo. Definiremos para ello una función llamada `muestra_entorno`. \n",
    "\n",
    "La lista de acciones se encuentra en `env.action_space`. Se trata de un objeto de tipo `Discrete(2)`, lo que nos indica que hay dos acciones diferentes, identificadas con 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f72b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "print(env.get_wrapper_attr('dealer'))\n",
    "print(env.get_wrapper_attr('player'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def muestra_entorno(env:gym.Env) -> None:\n",
    "    plt.imshow(env.render())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Probamos el método para mostrar el entorno\n",
    "muestra_entorno(env)\n",
    "\n",
    "# Muestra las acciones disponibles\n",
    "acciones = env.action_space\n",
    "print(type(acciones))\n",
    "print(f'Hay {acciones.n} acciones')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62fcd9",
   "metadata": {},
   "source": [
    "Podemos comprobar que hay dos acciones distintas. Vamos a ejecutar las acciones 0 y 1 para comprobar su efecto y examinar las variables que nos devuelve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78367b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "print('Situación antes:')\n",
    "print('Crupier:', env.get_wrapper_attr('dealer'))\n",
    "print('Jugador:', env.get_wrapper_attr('player'))\n",
    "\n",
    "# TODO - Cambia la acción a 0 o 1 y ejecuta la celda las veces que necesites hasta que identifiques qué acción corresponde con 0 y cuál con 1.\n",
    "accion = ...\n",
    "obs, reward, terminated, truncated, info = ...\n",
    "\n",
    "print('Obs:', obs)\n",
    "print('Reward:', reward)\n",
    "print('Terminated:', terminated)\n",
    "print('Truncated:', truncated)\n",
    "print('Info:', info)\n",
    "\n",
    "print('Situación después:')\n",
    "print('Crupier:', env.get_wrapper_attr('dealer'))\n",
    "print('Jugador:', env.get_wrapper_attr('player'))\n",
    "\n",
    "# TODO - Indica qué acción es 0 y cuál es 1.\n",
    "PLANTARSE = ...\n",
    "PEDIR_CARTA = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dcba06",
   "metadata": {},
   "source": [
    "### Representación del problema\n",
    "\n",
    "Además de identificar las acciones, hemos podido comprobar lo siguiente:\n",
    " - En `obs` está toda la información que necesita el agente. Se trata de una terna que indica, por este orden, la puntuación del jugador, la puntuación **visible** del crupier (solo su primera carta) y un booleano que indica si el jugador dispone de un as (que es especial porque puede contar 1 u 11).\n",
    " - `terminated` es True cuando termina la partida, es decir, cuando el jugador se planta o cuando pide una carta y se pasa de 21.\n",
    " - `reward` es -1 cuando el jugador pierde la partida (el crupier obtuvo un valor más cerca de 21 sin pasarse) y 1 cuando el jugador gana (situación inversa). Es 0 en cualquier otro caso.\n",
    " \n",
    "Por tanto, podemos representar los estados con la terna que nos ofrece la observación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b56b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# Representaremos los estados como tuplas con los siguientes campos con nombre:\n",
    "#  - puntuacion_jugador: Representa la suma de los valores de las cartas recibidas\n",
    "#  - puntuacion_visible_crupier: Representa la suma de las cartas visibles del crupier\n",
    "#  - jugador_con_as: Booleano que indica si el jugador tiene un as\n",
    "Estado = namedtuple('Estado', ['puntuacion_jugador', 'puntuacion_visible_crupier', 'jugador_con_as'])\n",
    "\n",
    "# TODO - Completa la función\n",
    "def get_estado(obs:tuple[int, int, int]) -> Estado:\n",
    "    ...\n",
    "\n",
    "# COMPROBACIONES - TODO - Verifica que el estado que se imprime coincide con lo mostrado en la imagen\n",
    "env.reset()\n",
    "obs = env.step(PLANTARSE)[0]\n",
    "estado = get_estado(obs)\n",
    "print(estado)\n",
    "muestra_entorno(env)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7034ab",
   "metadata": {},
   "source": [
    "### Ausencia de modelo del entorno\n",
    "\n",
    "Este problema es no determinista, es decir, aplicar una acción a un estado fijo no siempre da los mismos resultados. Veamos un ejemplo:\n",
    "> Supongamos que estamos en el estado $s=$(17, 1, True). Esto quiere decir lo siguiente:\n",
    "> - El crupier tiene un as (y otra carta que el agente no puede ver)\n",
    "> - El jugador tiene un as y un 6.\n",
    "> En esta situación, supongamos que el agente ejecuta la acción $a=$PLANTARSE. Hay dos posibilidades:\n",
    "> - El crupier acaba sumando más de 17 (pero menos de 22), así que gana. `reward` será $r=-1$ y el estado resultante  será $s'=$(17,X,True) con $X \\in [18,21]$.\n",
    "> - El crupier acaba pasándose, así que gana el agente. `reward` será $r=1$ y el estado será $s'=$(17,X,True) con $X > 21$.\n",
    "> De la misma manera, si el agente eligiese $a=$PEDIR_CARTA, el estado y la reward dependerían del azar.\n",
    "\n",
    "Dicho de otra manera, el agente no dispone de $p(s',r | s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ee3264",
   "metadata": {},
   "source": [
    "## Definición de políticas\n",
    "\n",
    "Vamos a definir una política aleatoria, es decir, que para cada para estado $s$ otorgue la misma probabilidad $\\pi(a|s)$ a cualquiera de las dos acciones. Para ello, siguiendo lo hecho en el laboratorio 4, declararemos un `numpy.array` que registre, para cada par estado-acción $s,a$, la probabilidad $\\pi(a|s)$ (que será la misma siempre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Inicializa la policy\n",
    "politica_aleatoria = np.zeros(...)\n",
    "politica_aleatoria[:] = ...\n",
    "\n",
    "# TODO - Recupera la función sample_policy del laboratorio 4 y adáptala para que devuelva el número de la acción\n",
    "...\n",
    "\n",
    "# COMPROBACIONES\n",
    "assert(politica_aleatoria[1,1,1,1]==0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2ed4c",
   "metadata": {},
   "source": [
    "Vamos a definir además una función para poder visualizar la política cómodamente. Para cada estado, nos indicará con un color si la acción a tomar será PLANTARSE o PEDIR CARTA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para mostrar la política\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "def dibuja_politica(politica: np.ndarray) -> None:\n",
    "    def crea_figura(con_as: bool, ax:Axes):\n",
    "        x_range = np.arange(1, 11)\n",
    "        y_range = np.arange(11, 22)\n",
    "        X, Y = np.meshgrid(x_range, y_range)\n",
    "        Z = politica[Y[::-1],X ,1 if con_as else 0,1]\n",
    "        surf = ax.imshow(Z, cmap=plt.get_cmap('Oranges'), vmin=0, vmax=1, extent=[0.5, 10.5, 10.5, 21.5])\n",
    "        plt.xticks(x_range, ('A', '2', '3', '4', '5', '6', '7', '8', '9', '10'))\n",
    "        plt.yticks(y_range)\n",
    "        ax.set_xlabel('Crupier')\n",
    "        ax.set_ylabel('Jugador')\n",
    "        #ax.grid(color='black', linestyle='-', linewidth=1)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "        cbar = plt.colorbar(surf, ticks=[0, 1], cax=cax)\n",
    "        cbar.ax.set_yticklabels(['PLANTARSE','PEDIR CARTA'])\n",
    "        cbar.ax.invert_yaxis() \n",
    "            \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.set_title('Sin as', fontsize=16)\n",
    "    crea_figura(False, ax)\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.set_title('Con as', fontsize=16)\n",
    "    crea_figura(True, ax)\n",
    "    plt.show()\n",
    "    \n",
    "# TODO - Muestra la política aleatoria declarada antes usando la función recién definida\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f0cfe",
   "metadata": {},
   "source": [
    "Como la política es aleatoria, se muestra con un color intermedio para cualquier estado.\n",
    "\n",
    "### Uso de la política\n",
    "\n",
    "Vamos a adaptar la función que genera un episodio siguiendo una política preestablecida, desarrollada en el laboratorio 4. Para los métodos Montecarlo necesitaremos registrar la secuencia $S_0,A_0,R_1,S_1,A_1,R_2...S_{T-1},A_{T-1},R_T$ de estados recorridos, acciones aplicadas y recompensas recibidas, así que deberemos adaptar la implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c6e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Recupera la función simula_episodio del laboratorio 4 y haz las modificaciones necesarias para que funcione en este problema.\n",
    "# Además, la función deberá ahora devolver también una lista de tuplas con los estados recorridos, acciones aplicadas y recompensas recibidas.\n",
    "def simula_episodio(politica:np.ndarray, muestra_renders:bool=False, imprime:bool=False) -> tuple[float, int, list[tuple[Estado, int, float]]]:\n",
    "    ...\n",
    "    return (ret, contador, recorrido)\n",
    "\n",
    "retorno, num_pasos, recorrido = simula_episodio(politica_aleatoria, imprime=True)\n",
    "\n",
    "print('Recorrido:', recorrido)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c33c8f3",
   "metadata": {},
   "source": [
    "### Comprobación del rendimiento de la política\n",
    "\n",
    "Hagamos un experimento para evaluar la eficacia de la política aleatoria. Simularemos 200 episodios, anotando los retornos obtenidos en cada uno.\n",
    "\n",
    "Mostraremos estadísticas y gráficos que nos puedan ayudar a tener una idea de cómo de bien funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Recupera la función comprueba política del lab4\n",
    "# Modifícala para que registre y muestre las recompensas en lugar del número de pasos\n",
    "# y úsala para probar la política aleatoria\n",
    "def comprueba_politica(politica:np.ndarray, imprime:bool = False) -> None:\n",
    "    ...\n",
    "    \n",
    "# Comprueba los retornos de la política aleatoria usando la función recién declarada\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dbfee0",
   "metadata": {},
   "source": [
    "Podrás comprobar que si se juega al Blackjack siguiendo una política aleatoria, la expectativa es que se va a perder bastante dinero.\n",
    "\n",
    "## Evaluación de políticas\n",
    "\n",
    "Para poder mejorar la política aleatoria en el laboratorio 4 determinábamos el valor $v_\\pi(s)$ de cada estado y, en función de él, desde cualquier estado $s$ elegíamos la acción que nos dirigía al estado que mayor retorno obtuviese. Sin embargo, para poder hacer eso a partir de $v_\\pi(s)$, es necesario conocer $p(s',r|s,a)$, y ese no es el caso en este problema. Si no sabemos a qué estado $s'$ conduce cada acción $a$, no podemos elegir la acción $a$ que dé más retorno.\n",
    "\n",
    "Por ello, los métodos Montecarlo optan por estimar $q_\\pi(s,a)$ o, lo que es lo mismo, el retorno esperado aplicando la política $\\pi$ de aplicar la acción $a$ en el estado $s$.\n",
    "\n",
    "Implementemos el siguiente algoritmo, pero calculando $q_\\pi(s,a)$ además de $v_\\pi(s)$.\n",
    "\n",
    "![MC prediction](./img/montecarlo-evaluation-algorithm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed4b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 1.0 # En este problema no descontaremos las recompensas futuras a la hora de calcular el retorno\n",
    "\n",
    "def mc_evaluate(politica:np.ndarray, iteraciones:int = 2000) -> tuple[np.ndarray, np.ndarray]:\n",
    "    #Initialize\n",
    "    # TODO - Indica la shape del array que almacena v(s) para cada estado s\n",
    "    vs = np.zeros(...)\n",
    "    # TODO - Indica la shape del array que almacena q(s,a) para cada par s,a\n",
    "    qsa = np.zeros(...)\n",
    "    \n",
    "    # En returns tendremos una lista de retornos (inicialmente vacía) por cada par s,a\n",
    "    retornos = []\n",
    "    # Creamos la estructura\n",
    "    for i in range(qsa.shape[0]):\n",
    "        r_i = []\n",
    "        for j in range(qsa.shape[1]):\n",
    "            r_j = []\n",
    "            for k in range(qsa.shape[2]):\n",
    "                r_k = []\n",
    "                for l in range(qsa.shape[3]):\n",
    "                    r_k.append([])\n",
    "                r_j.append(r_k)\n",
    "            r_i.append(r_j)\n",
    "        retornos.append(r_i)\n",
    "        \n",
    "    # Loop\n",
    "    # Bucle principal del algoritmo (solo el número de iteraciones que se haya indicado por parámetro) adaptándolo para calcular q(s,a) siguiendo la policy\n",
    "    for i in range(iteraciones):\n",
    "        # TODO - Obtén los valores de un episodio simulado siguiendo la policy\n",
    "        ep_retorno, ep_num_pasos, ep_recorrido = ...\n",
    "        g = 0\n",
    "        estados_restantes:list[Estado] = []\n",
    "        for estado, _, _ in ep_recorrido:\n",
    "            estados_restantes.append(estado)\n",
    "        \n",
    "        for estado, accion, recompensa in reversed(ep_recorrido):\n",
    "            estados_restantes.pop()\n",
    "            # TODO - Actualiza g según dicta el algoritmo\n",
    "            ...\n",
    "            if estado not in estados_restantes:\n",
    "                # TODO - Añade g a los retornos de este par estado-acción (calcularemos todas las medias al final)\n",
    "                ...\n",
    "    \n",
    "    # Ahora calculamos los retornos medios\n",
    "    for i in range(qsa.shape[0]):\n",
    "        for j in range(qsa.shape[1]):\n",
    "            for k in range(qsa.shape[2]):\n",
    "                for l in range(qsa.shape[3]):\n",
    "                    qsa[i,j,k,l] = np.mean(retornos[i][j][k][l])\n",
    "                    vs[i,j,k] += politica[i,j,k,l] * qsa[i,j,k,l]\n",
    "                    \n",
    "    return qsa, vs\n",
    "\n",
    "q_values_aleatoria, v_values_aleatoria = mc_evaluate(politica_aleatoria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd444ecb",
   "metadata": {},
   "source": [
    "Vamos a representar el valor de cada estado en una gráfica para visualizarlo cómodamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a representar en una gráfica v(s) para cada estado s\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def dibuja_vs(vs:np.ndarray) -> None:\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(121, projection='3d')\n",
    "    ax.set_title('Sin as', fontsize=16)\n",
    "    x = np.arange(vs.shape[0])\n",
    "    y = np.arange(vs.shape[1])\n",
    "\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    sin_ases = vs[X,Y,0]\n",
    "    ax.contour3D(X, Y, sin_ases, 50, cmap='binary')\n",
    "    ax.set_xlabel('player')\n",
    "    ax.set_ylabel('croupier')\n",
    "    ax.set_zlabel('v(s)');\n",
    "    \n",
    "\n",
    "    ax = fig.add_subplot(122, projection='3d')\n",
    "    ax.set_title('Con as', fontsize=16)\n",
    "    \n",
    "    x = np.arange(10,vs.shape[0])\n",
    "    y = np.arange(vs.shape[1])\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    con_ases = vs[X,Y,1]\n",
    "    ax.contour3D(X, Y, con_ases, 50, cmap='binary')\n",
    "    ax.set_xlabel('player')\n",
    "    ax.set_ylabel('croupier')\n",
    "    ax.set_zlabel('v(s)');\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "dibuja_vs(v_values_aleatoria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b9e0d",
   "metadata": {},
   "source": [
    "En las gráficas se puede apreciar lo siguiente:\n",
    " - Los valores asignados son 'irregulares', en el sentido de que estados contiguos tienen valores dispares y no hay una tendencia uniforme. Este efecto se debe a que $v_\\pi(s)$ se está estimando en función de los episodios que han recorrido el estado $s$; cuando el estado se recorre pocas veces, la estimación es poco fiable. Es por ello que el efecto se reduce si aumentamos el número de iteraciones.\n",
    " - La gráfica 'Con as' es más irregular que la 'Sin as'. Reflexionando un poco, podemos darnos cuenta de que las partidas en las que el jugador tiene un as son infrecuentes, con lo cual las estimaciones se hacen a partir de un número menor de muestras y, por tanto, son menos fiables.\n",
    " - Siguiendo la política aleatoria, los valores son más altos cuanto más cerca está el jugador de 21.\n",
    "\n",
    "Examinemos ahora nuestra estimación de $q_\\pi(s,a)$. Para ello miremos solo el valor para los estados en que el jugador tiene cartas que suman 8 (sin tener un as)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d95fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_values_aleatoria[8,:,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e14c72",
   "metadata": {},
   "source": [
    "Podemos ver, para cada posible estado de las cartas del croupier, el valor de ejecutar, respectivamente, las acciones PLANTARSE y PEDIR CARTA, siguiendo después la política aleatoria.\n",
    "\n",
    "Es de esperar que muchas de las estimaciones de $q_\\pi(s,a)$ de que disponemos sean aún menos precisas que las de $v_\\pi(s)$, debido al bajo número de muestras.\n",
    "\n",
    "\n",
    "### Mejora de la política\n",
    "\n",
    "Ahora que disponemos del valor de $q_\\pi(s,a)$ (aunque sea una mala estimación), podemos mejorar la política aleatoria y conseguir una política $\\pi'\\geq\\pi$ simplemente haciendo una selección *avara (greedy)* del valor $q_\\pi(s,a)$ para todas las acciones $a$ disponibles desde cualquier estado $s$. \n",
    "\n",
    "Implementemos esta mejora de la política."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d78d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Recupera la función greedify_policy del laboratorio 4 y adáptala para que trabaje con q(s,a) en lugar de v(s)\n",
    "def crea_politica_avara(q_values: np.ndarray) -> np.ndarray:\n",
    "    ...\n",
    "\n",
    "# Calculamos la nueva política a partir de los q_values que habíamos aproximado para la política aleatoria\n",
    "politica_mejorada = crea_politica_avara(q_values_aleatoria)\n",
    "dibuja_politica(politica_mejorada)\n",
    "comprueba_politica(politica_mejorada)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f30b64",
   "metadata": {},
   "source": [
    "Se puede comprobar que la nueva política obtiene mejores resultados que la política aleatoria. Sin embargo, al visualizarla observamos que es bastante 'irregular', lo cual era de esperar conociendo el problema que tenían nuestras estimaciones de $q_\\pi(s,a)$ (que utilizamos como fundamento para elegir la nueva política).\n",
    "\n",
    "### Mejoras sucesivas\n",
    "\n",
    "En el laboratorio 4, repetíamos este ciclo de evaluación de la política para obtener $v_\\pi(s)$, a partir del cual obtener una $\\pi'$ mejor, volver a evaluarla para obtener $v_{\\pi'}(s)$ y a partir de ella obtener una $\\pi''$ todavía mejor... y continuábamos este proceso mientras la política seguía mejorando.\n",
    "\n",
    "![Policy iteration](./img/policy-iteration.png)\n",
    "\n",
    "Podríamos intentar ese mismo proceso en este caso, pero hay un problema importante.\n",
    "\n",
    "Supongamos que nuestra política decide que para $s=[19, 7, 1]$ la acción $a$ que debe tomar es PLANTARSE. Al simular un episodio siguiendo esa política, **¡no estaremos tomando ninguna muestra del retorno de usar PEDIR CARTA en ese estado!**. Dicho de otra manera, tendremos una buena estimación de $q_\\pi([19, 7, 1], $PLANTARSE$)$, pero no tendremos estimación de $q_\\pi([19, 7, 1], $PEDIR CARTA$)$. Por tanto, no podremos calcular cuál es la mejor acción, es decir, $\\pi'([19, 7, 1]) = arg\\max_{a'} q_\\pi([19, 7, 1], a')$ y nuestra iteración de políticas se vería trucada.\n",
    "\n",
    "Esta es una manifestación del problema de **exploración vs. explotación** con la que se deben enfrentar los métodos de Montecarlo. Existen varias soluciones:\n",
    " - **Exploring starts**: Seleccionar el primer estado y la primera acción aleatoriamente (sin atender a la política) de entre las disponibles.\n",
    " - **Uso de políticas estocásticas**: mantener un nivel de aleatoriedad en la política para que explore algunas veces (políticas *$\\epsilon$-soft*).\n",
    " - **Exploración off-policy**: uso de una política para las simulaciones distinta de la que se está aprendiendo.\n",
    " \n",
    "En este laboratorio utilizaremos la segunda solución. Haremos las simulaciones siguiendo la política *greedy*, pero que tendrá una probabilidad $\\epsilon$ de tomar una acción aleatoria. Además, para acelerar el cálculo, haremos la evaluación y la mejora de la política en el mismo algoritmo (de manera similar a como ocurría en el algoritmo de iteración de valores visto en el laboratorio 4). El algoritmo a seguir es muy similar al algoritmo de evaluación visto anteriormente y está descrito a continuación:\n",
    "\n",
    "![Montecarlo ES](./img/mc-control-algorithm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe3e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Implementa la función basándote en la descripción del algoritmo (es muy similar a la función mc_evaluate vista más arriba)\n",
    "def mc_control(num_pasos:int=100000, epsilon:float=0.2) -> np.ndarray:\n",
    "    qsa = np.zeros(...)\n",
    "    politica = np.zeros(...)\n",
    "    # Partimos de la política aleatoria\n",
    "    politica[:] = 1.0 / NUM_ACCIONES\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return crea_politica_avara(politica)\n",
    "\n",
    "# Calculamos la política usando un epsilon de 0.4 para garantizar la exploración\n",
    "politica_montecarlo_es = mc_control(epsilon=0.4)\n",
    "dibuja_politica(politica_montecarlo_es)\n",
    "comprueba_politica(politica_montecarlo_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc59813",
   "metadata": {},
   "source": [
    "La política obtenida es mucho mejor, aproximándose ya a la política óptima para este problema. Prueba distintos valores de `epsilon` o de `num_pasos` para intentar conseguir una política aún mejor.\n",
    "\n",
    "### Política óptima\n",
    "\n",
    "Como referencia, vamos a aplicar la política que se describe en el libro \"Reinforcement Learning: An Introduction\" como óptima para este problema (calculada tras un alto número de iteraciones del algoritmo Montecarlo Exploring Starts, similar al que hemos aplicado nosotros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b228e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIT = [0.0, 1.0]\n",
    "STICK = [1.0, 0.0]\n",
    "barto = np.zeros(politica_montecarlo_es.shape)\n",
    "barto[:] = 0.5\n",
    "\n",
    "cutoffs0=[16,12,12,11,11,11,16,16,16,16]\n",
    "cutoffs1=[18,17,17,17,17,17,17,17,18,18]\n",
    "for i in range(len(cutoffs0)):\n",
    "    barto[0:cutoffs0[i] + 1, i + 1, 0] = HIT\n",
    "    barto[cutoffs0[i] + 1:, i + 1, 0] = STICK\n",
    "    barto[0:cutoffs1[i] + 1, i + 1, 1] = HIT\n",
    "    barto[cutoffs1[i] + 1:, i + 1, 1] = STICK\n",
    "    \n",
    "dibuja_politica(barto)\n",
    "comprueba_politica(barto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca7db6",
   "metadata": {},
   "source": [
    "Incluso con la política óptima, jugar al Blackjack no es una buena decisión financiera.\n",
    "\n",
    "## ¡Enhorabuena!\n",
    "Has completado el laboratorio 5, en el que hemos aplicado un método Montecarlo para aproximar la política óptima para un problema del que no disponemos de modelo.\n",
    "\n",
    "Si quieres profundizar más, intenta implementar el algoritmo de exploración off-policy visto en la hora de teoría."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
